{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12817559,"sourceType":"datasetVersion","datasetId":8087825},{"sourceId":12821797,"sourceType":"datasetVersion","datasetId":8108119},{"sourceId":12821835,"sourceType":"datasetVersion","datasetId":8108149},{"sourceId":12821850,"sourceType":"datasetVersion","datasetId":8108160},{"sourceId":12825538,"sourceType":"datasetVersion","datasetId":8110818},{"sourceId":12832934,"sourceType":"datasetVersion","datasetId":8105308}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports & helpers","metadata":{}},{"cell_type":"code","source":"import os, re, glob, random, shutil\nfrom glob import glob as gglob\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:29:19.695299Z","iopub.execute_input":"2025-08-22T09:29:19.695912Z","iopub.status.idle":"2025-08-22T09:29:31.352750Z","shell.execute_reply.started":"2025-08-22T09:29:19.695874Z","shell.execute_reply":"2025-08-22T09:29:31.352075Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 256\nBATCH    = 12\nEPOCHS   = 40\nSAVE_PATH = \"/kaggle/working/checkpoints\"\nos.makedirs(SAVE_PATH, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:29:31.354358Z","iopub.execute_input":"2025-08-22T09:29:31.354692Z","iopub.status.idle":"2025-08-22T09:29:31.450449Z","shell.execute_reply.started":"2025-08-22T09:29:31.354667Z","shell.execute_reply":"2025-08-22T09:29:31.449490Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def denorm(x):  # [-1,1] -> [0,1]\n    return (x + 1) / 2\n\ndef crop_like(src, tgt):\n    h, w = tgt.shape[-2], tgt.shape[-1]\n    sh, sw = src.shape[-2], src.shape[-1]\n    if (sh, sw) == (h, w): return src\n    dh = max((sh - h) // 2, 0); dw = max((sw - w) // 2, 0)\n    return src[..., dh:dh+h, dw:dw+w]\n\ndef dilate_mask(m, k=9):\n    # m: [B,1,H,W] in 0..1; dilation bằng max_pool để ăn hết viền occluder\n    if k < 3: return m\n    pad = (k - 1) // 2\n    return F.max_pool2d(m, kernel_size=k, stride=1, padding=pad)\n\ndef feather_mask(m, k=9):\n    if k < 3: return m\n    pad = (k - 1) // 2\n    return F.avg_pool2d(m, kernel_size=k, stride=1, padding=pad).clamp(0,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:29:31.451358Z","iopub.execute_input":"2025-08-22T09:29:31.451719Z","iopub.status.idle":"2025-08-22T09:29:31.478693Z","shell.execute_reply.started":"2025-08-22T09:29:31.451689Z","shell.execute_reply":"2025-08-22T09:29:31.478206Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"paths = [\n    '/kaggle/input/flickr-faces-hq/00000',\n    '/kaggle/input/flickr-faces-hq-2/01000',\n    '/kaggle/input/flickr-faces-hq-3/02000',\n    '/kaggle/input/flickr-faces-hq-4/03000',\n    '/kaggle/input/flickr-faces-hq-5/04000'\n]\nDATA_DIR = '/kaggle/working/all_images'\nos.makedirs(DATA_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:29:31.480519Z","iopub.execute_input":"2025-08-22T09:29:31.480792Z","iopub.status.idle":"2025-08-22T09:29:31.496603Z","shell.execute_reply.started":"2025-08-22T09:29:31.480775Z","shell.execute_reply":"2025-08-22T09:29:31.496139Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"for p in paths:\n    for ext in (\"png\",\"jpg\",\"jpeg\"):\n        for f in gglob(f\"{p}/*.{ext}\"):\n            dst = os.path.join(DATA_DIR, os.path.basename(f))\n            if not os.path.exists(dst): shutil.copy(f, dst)\nprint(\"images:\", len(gglob(DATA_DIR+\"/*\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:29:31.497386Z","iopub.execute_input":"2025-08-22T09:29:31.497633Z","iopub.status.idle":"2025-08-22T09:32:36.211469Z","shell.execute_reply.started":"2025-08-22T09:29:31.497611Z","shell.execute_reply":"2025-08-22T09:32:36.210731Z"}},"outputs":[{"name":"stdout","text":"images: 5000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Dataset with random occlusion","metadata":{}},{"cell_type":"code","source":"class FaceOcclusionDataset(Dataset):\n    def __init__(self, folder, transform=None):\n        self.files = sorted([f for f in gglob(f\"{folder}/*\")\n                             if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n        self.transform = transform\n\n    def __len__(self): return len(self.files)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.files[idx]).convert(\"RGB\")\n        if self.transform: img = self.transform(img)      # [-1,1], [3,H,W]\n        occ, mask = self.occlude_with_mask(img.clone())   # mask [1,H,W]\n        # bbox cho local D (padding nhẹ)\n        ys, xs = torch.where(mask[0] > 0)\n        if len(ys)==0:\n            y1,x1,y2,x2 = 0,0,img.shape[1],img.shape[2]\n        else:\n            y1,y2 = int(ys.min()), int(ys.max())+1\n            x1,x2 = int(xs.min()), int(xs.max())+1\n            pad = max((y2-y1)//8, 8)\n            y1, x1 = max(0, y1-pad), max(0, x1-pad)\n            y2, x2 = min(img.shape[1], y2+pad), min(img.shape[2], x2+pad)\n        bbox = torch.tensor([y1,x1,y2,x2], dtype=torch.int64)\n        return occ, img, mask.float(), bbox\n\n    def occlude_with_mask(self, img):\n        _, h, w = img.shape\n        min_r, max_r = int(0.14*h), int(0.30*h)\n        rh = random.randint(min_r, max_r); rw = random.randint(min_r, max_r)\n        y  = random.randint(rh, h - rh - 1); x  = random.randint(rw, w - rw - 1)\n        \n        if random.random() < 0.45:  c = torch.tensor([0.25,0.75,0.80])\n        elif random.random() < 0.5: gray = random.uniform(0.10,0.30); c = torch.tensor([gray,gray,gray])\n        else:                       c = torch.rand(3)*0.7 + 0.15\n        cmap = (c*2-1).view(3,1,1).expand_as(img)\n        # shape\n        shape = random.choice([\"circle\",\"rect\",\"ellipse\"])\n        mask = torch.zeros(h, w, dtype=torch.bool)\n        if shape == \"rect\":\n            y1,y2 = max(0,y-rh), min(h,y+rh); x1,x2 = max(0,x-rw), min(w,x+rw)\n            mask[y1:y2, x1:x2] = True\n        else:\n            yy, xx = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n            if shape == \"circle\":\n                r = min(rh, rw); mask = (xx-x).pow(2) + (yy-y).pow(2) <= r*r\n            else:\n                mask = ((xx.float()-x)/rw).pow(2) + ((yy.float()-y)/rh).pow(2) <= 1.0\n        occ = torch.where(mask.unsqueeze(0), cmap, img)\n        return occ, mask.unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.212413Z","iopub.execute_input":"2025-08-22T09:32:36.212685Z","iopub.status.idle":"2025-08-22T09:32:36.224243Z","shell.execute_reply.started":"2025-08-22T09:32:36.212655Z","shell.execute_reply":"2025-08-22T09:32:36.223480Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.224850Z","iopub.execute_input":"2025-08-22T09:32:36.225008Z","iopub.status.idle":"2025-08-22T09:32:36.246417Z","shell.execute_reply.started":"2025-08-22T09:32:36.224995Z","shell.execute_reply":"2025-08-22T09:32:36.245851Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dataset = FaceOcclusionDataset(DATA_DIR, transform_train)\nloader  = DataLoader(dataset, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.247064Z","iopub.execute_input":"2025-08-22T09:32:36.247337Z","iopub.status.idle":"2025-08-22T09:32:36.277804Z","shell.execute_reply.started":"2025-08-22T09:32:36.247321Z","shell.execute_reply":"2025-08-22T09:32:36.277348Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Models: OccNet (seg) + InpaintG (GatedConv ResUNet) + Dg/Dl","metadata":{}},{"cell_type":"code","source":"class GatedConv2d(nn.Module):\n    def __init__(self, in_c, out_c, k=3, s=1, p=1):\n        super().__init__()\n        self.feat = nn.Conv2d(in_c, out_c, k, s, p)\n        self.gate = nn.Conv2d(in_c, out_c, k, s, p)\n        self.bn   = nn.BatchNorm2d(out_c)\n    def forward(self, x):\n        f = self.feat(x)\n        g = torch.sigmoid(self.gate(x))\n        return F.relu(self.bn(f) * g, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.278413Z","iopub.execute_input":"2025-08-22T09:32:36.278600Z","iopub.status.idle":"2025-08-22T09:32:36.283469Z","shell.execute_reply.started":"2025-08-22T09:32:36.278585Z","shell.execute_reply":"2025-08-22T09:32:36.282691Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ConvBNReLU(nn.Module):\n    def __init__(self, i,o,k=3,s=1,p=1):\n        super().__init__()\n        self.m = nn.Sequential(nn.Conv2d(i,o,k,s,p), nn.BatchNorm2d(o), nn.ReLU(True))\n    def forward(self,x): return self.m(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.285583Z","iopub.execute_input":"2025-08-22T09:32:36.285884Z","iopub.status.idle":"2025-08-22T09:32:36.302923Z","shell.execute_reply.started":"2025-08-22T09:32:36.285870Z","shell.execute_reply":"2025-08-22T09:32:36.302356Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class OccNet(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.d1 = nn.Sequential(ConvBNReLU(3,64,4,2,1), ConvBNReLU(64,64))\n        self.d2 = nn.Sequential(ConvBNReLU(64,128,4,2,1), ConvBNReLU(128,128))\n        self.d3 = nn.Sequential(ConvBNReLU(128,256,4,2,1), ConvBNReLU(256,256))\n        self.u2 = nn.Sequential(nn.ConvTranspose2d(256,128,4,2,1), ConvBNReLU(128,128))\n        self.u1 = nn.Sequential(nn.ConvTranspose2d(256,64,4,2,1),  ConvBNReLU(64,64))\n        self.u0 = nn.Sequential(nn.ConvTranspose2d(128,32,4,2,1),  ConvBNReLU(32,32))\n        self.out = nn.Conv2d(32, 1, 1)\n    def forward(self,x):\n        e1 = self.d1(x); e2 = self.d2(e1); e3 = self.d3(e2)\n        d2 = self.u2(e3); d2 = torch.cat([crop_like(e2,d2), crop_like(d2,e2)],1)\n        d1 = self.u1(d2); d1 = torch.cat([crop_like(e1,d1), crop_like(d1,e1)],1)\n        d0 = self.u0(d1)\n        m  = torch.sigmoid(self.out(crop_like(d0,x)))   # [B,1,H,W]\n        return m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.303631Z","iopub.execute_input":"2025-08-22T09:32:36.304313Z","iopub.status.idle":"2025-08-22T09:32:36.321890Z","shell.execute_reply.started":"2025-08-22T09:32:36.304292Z","shell.execute_reply":"2025-08-22T09:32:36.321381Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class ResGatedBlock(nn.Module):\n    def __init__(self,c):\n        super().__init__()\n        self.b = nn.Sequential(GatedConv2d(c,c,3,1,1), GatedConv2d(c,c,3,1,1))\n    def forward(self,x): return self.b(x) + x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.322479Z","iopub.execute_input":"2025-08-22T09:32:36.322650Z","iopub.status.idle":"2025-08-22T09:32:36.351380Z","shell.execute_reply.started":"2025-08-22T09:32:36.322636Z","shell.execute_reply":"2025-08-22T09:32:36.350874Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class InpaintG(nn.Module):\n    def __init__(self, in_ch=4):  # 3 RGB + 1 mask\n        super().__init__()\n        def down(i,o): return nn.Sequential(GatedConv2d(i,o,4,2,1))\n        def up(i,o):   return nn.Sequential(nn.ConvTranspose2d(i,o,4,2,1),\n                                           nn.BatchNorm2d(o), nn.ReLU(True))\n        self.enc1 = down(in_ch,64); self.enc2 = down(64,128); self.enc3 = down(128,256)\n        self.res  = nn.Sequential(ResGatedBlock(256), ResGatedBlock(256))\n        self.dec3 = up(256,128); self.dec2 = up(128,64); self.dec1 = up(64,32)\n        self.fuse3= nn.Sequential(GatedConv2d(128+128,128))\n        self.fuse2= nn.Sequential(GatedConv2d(64+64,64))\n        self.out  = nn.Conv2d(32,3,3,1,1)\n        self.tanh = nn.Tanh()\n    def forward(self, x_rgb, m_pred):\n        x = torch.cat([x_rgb, m_pred], dim=1)\n        e1=self.enc1(x); e2=self.enc2(e1); e3=self.enc3(e2); r=self.res(e3)\n        d3=self.dec3(r); d3=self.fuse3(torch.cat([crop_like(e2,d3), crop_like(d3,e2)],1))\n        d2=self.dec2(d3); d2=self.fuse2(torch.cat([crop_like(e1,d2), crop_like(d2,e1)],1))\n        d1=crop_like(self.dec1(d2), x_rgb)\n        return self.tanh(self.out(d1))   # full clean image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.351983Z","iopub.execute_input":"2025-08-22T09:32:36.352219Z","iopub.status.idle":"2025-08-22T09:32:36.368597Z","shell.execute_reply.started":"2025-08-22T09:32:36.352201Z","shell.execute_reply":"2025-08-22T09:32:36.368076Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class PatchDiscriminator(nn.Module):\n    def __init__(self, in_c=3, ndf=64):\n        super().__init__()\n        SN = nn.utils.spectral_norm\n        def C(i,o,s=2): return nn.Sequential(SN(nn.Conv2d(i,o,4,s,1)),\n                                             nn.LeakyReLU(0.2,True))\n        self.net = nn.Sequential(C(in_c,ndf,2), C(ndf,ndf*2,2), C(ndf*2,ndf*4,2),\n                                 SN(nn.Conv2d(ndf*4,1,4,1,1)))\n    def forward(self,x): return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.369118Z","iopub.execute_input":"2025-08-22T09:32:36.369275Z","iopub.status.idle":"2025-08-22T09:32:36.389883Z","shell.execute_reply.started":"2025-08-22T09:32:36.369262Z","shell.execute_reply":"2025-08-22T09:32:36.389325Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"Gm = OccNet().to(DEVICE)            # mask predictor\nG  = InpaintG().to(DEVICE)          # generator\nDg = PatchDiscriminator().to(DEVICE)  # global\nDl = PatchDiscriminator().to(DEVICE)  # local","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.390623Z","iopub.execute_input":"2025-08-22T09:32:36.390853Z","iopub.status.idle":"2025-08-22T09:32:36.768916Z","shell.execute_reply.started":"2025-08-22T09:32:36.390830Z","shell.execute_reply":"2025-08-22T09:32:36.768348Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Losses, EMA & optimizers","metadata":{}},{"cell_type":"code","source":"def d_hinge(r, f): return F.relu(1.-r).mean() + F.relu(1.+f).mean()\ndef g_hinge(f):     return -f.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.769691Z","iopub.execute_input":"2025-08-22T09:32:36.769917Z","iopub.status.idle":"2025-08-22T09:32:36.773977Z","shell.execute_reply.started":"2025-08-22T09:32:36.769899Z","shell.execute_reply":"2025-08-22T09:32:36.773423Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def laplace(x):\n    k = torch.tensor([[0,1,0],[1,-4,1],[0,1,0]], dtype=x.dtype, device=x.device).view(1,1,3,3)\n    k = k.repeat(3,1,1,1)\n    return F.conv2d(x, k, padding=1, groups=3)\ndef edge_loss(fake, real): return F.l1_loss(laplace(fake), laplace(real))\n\ndef tv_loss(x):\n    return (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean() + \\\n           (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().mean()\n\ndef dice_loss(pred, target, eps=1e-6):\n    inter = (pred*target).sum(dim=(1,2,3))\n    union = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n    dice = (2*inter + eps) / (union + eps)\n    return 1 - dice.mean()\n\ndef symmetry_loss(img, mask):\n    img_f = torch.flip(img, dims=[3])\n    mask_f= torch.flip(mask, dims=[3])\n    w = (mask * (1-mask_f)).expand_as(img)\n    return (w * (img - img_f).abs()).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.774695Z","iopub.execute_input":"2025-08-22T09:32:36.774912Z","iopub.status.idle":"2025-08-22T09:32:36.792504Z","shell.execute_reply.started":"2025-08-22T09:32:36.774889Z","shell.execute_reply":"2025-08-22T09:32:36.791945Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"USE_PERCEPTUAL = True\nperc_lambda = 0.1\ntry:\n    from torchvision.models import vgg19, VGG19_Weights\n    _vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features.to(DEVICE).eval()\n    for p in _vgg.parameters(): p.requires_grad = False\n    VGG_READY = True\n    vgg_layers = [3, 8, 17]\n    def perceptual_feats(x01):\n        feats=[]; z=x01\n        for i,layer in enumerate(_vgg):\n            z=layer(z)\n            if i in vgg_layers: feats.append(z)\n        return feats\n    print(\"Perceptual ON\")\nexcept Exception as e:\n    VGG_READY=False; USE_PERCEPTUAL=False\n    print(\"Perceptual OFF:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:36.793299Z","iopub.execute_input":"2025-08-22T09:32:36.793494Z","iopub.status.idle":"2025-08-22T09:32:41.825197Z","shell.execute_reply.started":"2025-08-22T09:32:36.793472Z","shell.execute_reply":"2025-08-22T09:32:41.824582Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:03<00:00, 188MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Perceptual ON\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"class EMA:\n    def __init__(self, model, decay=0.999):\n        self.decay = decay\n        self.param_shadow = {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n        self.buff_shadow  = {n: b.detach().clone() for n,b in model.named_buffers() if torch.is_floating_point(b)}\n    @torch.no_grad()\n    def update(self, model):\n        for n,p in model.named_parameters():\n            if not p.requires_grad or n not in self.param_shadow: continue\n            self.param_shadow[n].mul_(self.decay).add_(p.detach(), alpha=1-self.decay)\n        for n,b in model.named_buffers():\n            if not torch.is_floating_point(b) or n not in self.buff_shadow: continue\n            self.buff_shadow[n].mul_(self.decay).add_(b.detach(), alpha=1-self.decay)\n    @torch.no_grad()\n    def copy_to(self, model):\n        for n,p in model.named_parameters():\n            if p.requires_grad and n in self.param_shadow:\n                p.data.copy_(self.param_shadow[n])\n        for n,b in model.named_buffers():\n            if torch.is_floating_point(b) and n in self.buff_shadow:\n                b.data.copy_(self.buff_shadow[n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:41.825943Z","iopub.execute_input":"2025-08-22T09:32:41.826227Z","iopub.status.idle":"2025-08-22T09:32:41.834644Z","shell.execute_reply.started":"2025-08-22T09:32:41.826202Z","shell.execute_reply":"2025-08-22T09:32:41.833844Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"emaG = EMA(G, decay=0.999)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:41.835361Z","iopub.execute_input":"2025-08-22T09:32:41.835619Z","iopub.status.idle":"2025-08-22T09:32:42.142736Z","shell.execute_reply.started":"2025-08-22T09:32:41.835596Z","shell.execute_reply":"2025-08-22T09:32:42.142228Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"opt_Gm = torch.optim.Adam(Gm.parameters(), lr=2e-4, betas=(0.5,0.999))\nopt_G  = torch.optim.Adam(G.parameters(),  lr=2e-4, betas=(0.5,0.999))\nopt_Dg = torch.optim.Adam(Dg.parameters(), lr=1e-4, betas=(0.5,0.999))  \nopt_Dl = torch.optim.Adam(Dl.parameters(), lr=1e-4, betas=(0.5,0.999))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:42.143452Z","iopub.execute_input":"2025-08-22T09:32:42.143710Z","iopub.status.idle":"2025-08-22T09:32:42.149362Z","shell.execute_reply.started":"2025-08-22T09:32:42.143685Z","shell.execute_reply":"2025-08-22T09:32:42.148827Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# weights\nlambda_rec_in, lambda_rec_out = 12.0, 1.0\nlambda_edge, lambda_tv, lambda_sym = 0.1, 0.05, 0.2\nlambda_mask_bce, lambda_mask_dice = 1.0, 1.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:42.150054Z","iopub.execute_input":"2025-08-22T09:32:42.150866Z","iopub.status.idle":"2025-08-22T09:32:42.165259Z","shell.execute_reply.started":"2025-08-22T09:32:42.150846Z","shell.execute_reply":"2025-08-22T09:32:42.164766Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def crop_local(x, bboxes, out_size=128):\n    B,C,H,W = x.shape; crops=[]\n    for b in range(B):\n        y1,x1,y2,x2 = map(int, bboxes[b].tolist())\n        y1=max(0,min(y1,H-2)); x1=max(0,min(x1,W-2))\n        y2=max(y1+1,min(y2,H)); x2=max(x1+1,min(x2,W))\n        patch = x[b:b+1,:, y1:y2, x1:x2]\n        patch = F.interpolate(patch, size=(out_size,out_size), mode='bilinear', align_corners=False)\n        crops.append(patch)\n    return torch.cat(crops,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:42.165999Z","iopub.execute_input":"2025-08-22T09:32:42.166209Z","iopub.status.idle":"2025-08-22T09:32:42.181916Z","shell.execute_reply.started":"2025-08-22T09:32:42.166194Z","shell.execute_reply":"2025-08-22T09:32:42.181219Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def bboxes_from_mask(mask, fallback_bbox, thr=0.4):\n    # mask: [B,1,H,W]\n    B,_,H,W = mask.shape\n    boxes=[]\n    for b in range(B):\n        ys,xs = torch.where(mask[b,0] > thr)\n        if len(ys)==0:\n            boxes.append(fallback_bbox[b])\n        else:\n            y1,y2 = int(ys.min()), int(ys.max())+1\n            x1,x2 = int(xs.min()), int(xs.max())+1\n            pad = max((y2-y1)//8, 8)\n            y1,x1 = max(0,y1-pad), max(0,x1-pad)\n            y2,x2 = min(H,y2+pad), min(W,x2+pad)\n            boxes.append(torch.tensor([y1,x1,y2,x2], device=mask.device))\n    return torch.stack(boxes,0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:42.182617Z","iopub.execute_input":"2025-08-22T09:32:42.182886Z","iopub.status.idle":"2025-08-22T09:32:42.204907Z","shell.execute_reply.started":"2025-08-22T09:32:42.182868Z","shell.execute_reply":"2025-08-22T09:32:42.204461Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Training ","metadata":{}},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    for it, (damaged, real, mask_gt, bbox_gt) in enumerate(loader):\n        damaged = damaged.to(DEVICE); real = real.to(DEVICE)\n        mask_gt = mask_gt.to(DEVICE); bbox_gt = bbox_gt.to(DEVICE)\n\n        # ---- 1) predict mask ----\n        mask_pred = Gm(damaged)                           # [B,1,H,W]\n        mask_dil  = dilate_mask(mask_pred.detach(), k=9).clamp(0,1)  \n\n        # ---- 2) generator ----\n        fake = G(damaged, mask_dil)                      \n\n        # ---- 3) recon & aux losses ----\n        rec_in  = (mask_gt * (fake - real).abs()).mean()\n        rec_out = ((1.0-mask_gt) * (fake - real).abs()).mean()\n        e_loss  = edge_loss(fake, real)\n        t_loss  = tv_loss(fake * mask_gt)\n        s_loss  = symmetry_loss(fake, mask_gt)\n        m_bce   = F.binary_cross_entropy(mask_pred, mask_gt)\n        m_dice  = dice_loss(mask_pred, mask_gt)\n\n        loss_recon = lambda_rec_in*rec_in + lambda_rec_out*rec_out \\\n                     + lambda_edge*e_loss + lambda_tv*t_loss + lambda_sym*s_loss \\\n                     + lambda_mask_bce*m_bce + lambda_mask_dice*m_dice\n\n        \n        use_gan = epoch >= 3\n        if use_gan:\n            # update D (global + local)\n            with torch.no_grad():\n                fake_det = fake\n            loss_Dg = d_hinge(Dg(real), Dg(fake_det.detach()))\n            rl = crop_local(real, bbox_gt)\n            fl = crop_local(fake_det.detach(), bbox_gt)\n            loss_Dl = d_hinge(Dl(rl), Dl(fl))\n            opt_Dg.zero_grad(); loss_Dg.backward(); opt_Dg.step()\n            opt_Dl.zero_grad(); loss_Dl.backward(); opt_Dl.step()\n\n            # adv for G \n            bbox_pred = bboxes_from_mask(mask_pred.detach(), bbox_gt)\n            g_adv = g_hinge(Dg(fake)) + g_hinge(Dl(crop_local(fake, bbox_pred)))\n        else:\n            g_adv = torch.tensor(0.0, device=DEVICE)\n\n        # perceptual \n        if USE_PERCEPTUAL and VGG_READY:\n            f_feats = perceptual_feats(denorm(fake))\n            r_feats = perceptual_feats(denorm(real))\n            p_loss  = sum(F.l1_loss(f, r) for f, r in zip(f_feats, r_feats))\n        else:\n            p_loss = torch.tensor(0.0, device=DEVICE)\n\n        loss_G_total = loss_recon + p_loss*perc_lambda + g_adv\n\n        # ---- 5) update G & Gm ----\n        opt_G.zero_grad(); opt_Gm.zero_grad()\n        loss_G_total.backward()\n        opt_G.step(); opt_Gm.step()\n        emaG.update(G)\n\n        if it % 100 == 0:\n            print(f\"[E{epoch}/{EPOCHS}] it{it}: \"\n                  f\"G={loss_G_total.item():.3f} | rin={rec_in.item():.3f} \"\n                  f\"rout={rec_out.item():.3f} edge={e_loss.item():.3f} \"\n                  f\"tv={t_loss.item():.3f} sym={s_loss.item():.3f} \"\n                  f\"mbce={m_bce.item():.3f} mdice={m_dice.item():.3f} \"\n                  f\"adv={g_adv.item():.3f}\")\n            grid = torch.cat([damaged[:4], fake[:4], real[:4]], 0)\n            save_image(denorm(grid), f\"{SAVE_PATH}/sample_{epoch}_{it}.png\", nrow=4)\n\n    torch.save(Gm.state_dict(), f\"{SAVE_PATH}/Occ_epoch{epoch}.pth\")\n    torch.save(G.state_dict(),  f\"{SAVE_PATH}/G_epoch{epoch}.pth\")\n    torch.save(Dg.state_dict(), f\"{SAVE_PATH}/Dg_epoch{epoch}.pth\")\n    torch.save(Dl.state_dict(), f\"{SAVE_PATH}/Dl_epoch{epoch}.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:32:42.205575Z","iopub.execute_input":"2025-08-22T09:32:42.205798Z","execution_failed":"2025-08-22T09:33:27.795Z"}},"outputs":[{"name":"stdout","text":"[E0/40] it0: G=3.362 | rin=0.090 rout=0.498 edge=0.972 tv=0.105 sym=0.029 mbce=0.755 mdice=0.764 adv=0.000\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Load EMA for inference","metadata":{}},{"cell_type":"code","source":"def extract_epoch(p):\n    m=re.search(r'_(\\d+)\\.pth$', p); return int(m.group(1)) if m else -1\ng_files  = sorted(glob.glob(f\"{SAVE_PATH}/G_epoch*.pth\"), key=extract_epoch)\ngm_files = sorted(glob.glob(f\"{SAVE_PATH}/Occ_epoch*.pth\"), key=extract_epoch)\nif g_files:  G.load_state_dict(torch.load(g_files[-1], map_location=DEVICE))\nif gm_files: Gm.load_state_dict(torch.load(gm_files[-1], map_location=DEVICE))\nemaG.copy_to(G)\nG.eval(); Gm.eval()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-22T09:33:27.795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference ","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/output\", exist_ok=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-22T09:33:27.795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reconstruct_image(gm, g, input_path, out_path, k_dilate=11, k_feather=11):\n    imgPIL = Image.open(input_path).convert(\"RGB\")\n    tfm = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ])\n    t = tfm(imgPIL).unsqueeze(0).to(DEVICE)  # [-1,1]\n\n    with torch.no_grad():\n        m = gm(t)                                        # [B,1,H,W]\n        m = dilate_mask(m, k=k_dilate).clamp(0,1)\n        fake = g(t, m)                                   \n        m_soft = feather_mask(m, k=k_feather)            \n        final = fake * m_soft + t * (1.0 - m_soft)       \n\n    save_image(denorm(final), out_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-22T09:33:27.795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_inp = '/kaggle/input/testdata/test_3.png'   \ntest_out = '/kaggle/working/output/reconstructed.png'\nreconstruct_image(Gm, G, test_inp, test_out, k_dilate=13, k_feather=13)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-22T09:33:27.795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(10,5))\nax[0].imshow(Image.open(test_inp)); ax[0].set_title(\"Input\");  ax[0].axis(\"off\")\nax[1].imshow(Image.open(test_out)); ax[1].set_title(\"Output\"); ax[1].axis(\"off\")\nplt.tight_layout(); plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-22T09:33:27.795Z"}},"outputs":[],"execution_count":null}]}