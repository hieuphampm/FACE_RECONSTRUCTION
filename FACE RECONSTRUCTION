{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12817559,"sourceType":"datasetVersion","datasetId":8087825},{"sourceId":12821797,"sourceType":"datasetVersion","datasetId":8108119},{"sourceId":12821835,"sourceType":"datasetVersion","datasetId":8108149},{"sourceId":12821850,"sourceType":"datasetVersion","datasetId":8108160},{"sourceId":12827661,"sourceType":"datasetVersion","datasetId":8105308},{"sourceId":12825538,"sourceType":"datasetVersion","datasetId":8110818}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b4442665-b9ef-42af-8d5c-62cedf6634b7","cell_type":"code","source":"import os, re, glob, random, shutil\nfrom glob import glob as gglob\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:31:25.054914Z","iopub.execute_input":"2025-08-21T13:31:25.055662Z","iopub.status.idle":"2025-08-21T13:31:32.437289Z","shell.execute_reply.started":"2025-08-21T13:31:25.055626Z","shell.execute_reply":"2025-08-21T13:31:32.436698Z"}},"outputs":[],"execution_count":1},{"id":"b3ad1448-1134-4e55-803b-f9c0c7c0caf5","cell_type":"code","source":"DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 256\nBATCH    = 16\nEPOCHS   = 30","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:31:32.438372Z","iopub.execute_input":"2025-08-21T13:31:32.438859Z","iopub.status.idle":"2025-08-21T13:31:32.526891Z","shell.execute_reply.started":"2025-08-21T13:31:32.438827Z","shell.execute_reply":"2025-08-21T13:31:32.526144Z"}},"outputs":[],"execution_count":2},{"id":"5fdc934b-bfd0-44db-b5b5-1161058f7562","cell_type":"code","source":"def denorm(x):  # [-1,1] -> [0,1]\n    return (x + 1) / 2\n\ndef crop_like(src, tgt):\n    h, w = tgt.shape[-2], tgt.shape[-1]\n    sh, sw = src.shape[-2], src.shape[-1]\n    if (sh, sw) == (h, w): return src\n    dh = max((sh - h) // 2, 0)\n    dw = max((sw - w) // 2, 0)\n    return src[..., dh:dh+h, dw:dw+w]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:31:32.527752Z","iopub.execute_input":"2025-08-21T13:31:32.528135Z","iopub.status.idle":"2025-08-21T13:31:32.543785Z","shell.execute_reply.started":"2025-08-21T13:31:32.528109Z","shell.execute_reply":"2025-08-21T13:31:32.543230Z"}},"outputs":[],"execution_count":3},{"id":"5a7bd3e9-bd7b-469d-b5c0-911cf92bbf95","cell_type":"markdown","source":"# Datasets","metadata":{}},{"id":"87ddcc9f-7ffe-488f-8f2f-7ffd76478068","cell_type":"code","source":"paths = [\n    '/kaggle/input/flickr-faces-hq/00000',\n    '/kaggle/input/flickr-faces-hq-2/01000',\n    '/kaggle/input/flickr-faces-hq-3/02000',\n    '/kaggle/input/flickr-faces-hq-4/03000',\n    '/kaggle/input/flickr-faces-hq-5/04000'\n]\nDATA_DIR = '/kaggle/working/all_images'\nos.makedirs(DATA_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:31:32.545486Z","iopub.execute_input":"2025-08-21T13:31:32.545665Z","iopub.status.idle":"2025-08-21T13:31:32.556704Z","shell.execute_reply.started":"2025-08-21T13:31:32.545651Z","shell.execute_reply":"2025-08-21T13:31:32.556058Z"}},"outputs":[],"execution_count":4},{"id":"2b72e8ab-ab33-40a9-926e-52da771a5e77","cell_type":"code","source":"copied = 0\nfor p in paths:\n    for ext in (\"png\", \"jpg\", \"jpeg\"):\n        for f in gglob(f\"{p}/*.{ext}\"):\n            dst = os.path.join(DATA_DIR, os.path.basename(f))\n            if not os.path.exists(dst):\n                shutil.copy(f, dst)\n                copied += 1\nprint(f\"Copied/ready images: {len(gglob(DATA_DIR+'/*'))} (newly copied: {copied})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:31:32.557500Z","iopub.execute_input":"2025-08-21T13:31:32.557743Z","iopub.status.idle":"2025-08-21T13:33:57.636727Z","shell.execute_reply.started":"2025-08-21T13:31:32.557727Z","shell.execute_reply":"2025-08-21T13:33:57.635885Z"}},"outputs":[{"name":"stdout","text":"Copied/ready images: 5000 (newly copied: 5000)\n","output_type":"stream"}],"execution_count":5},{"id":"80773925-4114-49a2-99a4-037b79e05a06","cell_type":"markdown","source":"# Dataset with random occlusion ","metadata":{}},{"id":"ef81e1fe-ae58-4fc9-a9f5-b5215009ed7f","cell_type":"code","source":"class FaceOcclusionDataset(Dataset):\n    def __init__(self, folder, transform=None):\n        self.files = sorted([f for f in gglob(f\"{folder}/*\") if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n        self.transform = transform\n\n    def __len__(self): return len(self.files)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.files[idx]).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)    # [-1,1], [3,H,W]\n        occluded, mask = self.occlude_with_mask(img.clone())  # mask [1,H,W] in {0,1}\n        return occluded, img, mask.float()\n\n    def occlude_with_mask(self, img):\n        _, h, w = img.shape\n        min_r, max_r = int(0.12*h), int(0.28*h)\n        rh = random.randint(min_r, max_r)\n        rw = random.randint(min_r, max_r)\n        y  = random.randint(rh, h - rh - 1)\n        x  = random.randint(rw, w - rw - 1)\n\n        if random.random() < 0.35:\n            c = torch.tensor([0.25, 0.75, 0.80])  # cyan-ish\n        elif random.random() < 0.5:\n            gray = random.uniform(0.15, 0.35); c = torch.tensor([gray,gray,gray])\n        else:\n            c = torch.rand(3) * 0.7 + 0.15\n        color_map = (c*2 - 1).view(3,1,1).expand_as(img)  # [3,H,W]\n\n        # mask 2D\n        shape = random.choice([\"rect\",\"circle\",\"ellipse\"])\n        mask = torch.zeros(h, w, dtype=torch.bool)\n\n        if shape == \"rect\":\n            y1, y2 = max(0, y-rh), min(h, y+rh)\n            x1, x2 = max(0, x-rw), min(w, x+rw)\n            mask[y1:y2, x1:x2] = True\n        else:\n            yy, xx = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n            if shape == \"circle\":\n                r = min(rh, rw)\n                mask = (xx - x).pow(2) + (yy - y).pow(2) <= r*r\n            else:  # ellipse\n                mask = ((xx.float()-x)/rw).pow(2) + ((yy.float()-y)/rh).pow(2) <= 1.0\n        img_occ = torch.where(mask.unsqueeze(0), color_map, img)  # [3,H,W]\n        return img_occ, mask.unsqueeze(0)  # [1,H,W]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:57.637659Z","iopub.execute_input":"2025-08-21T13:33:57.638302Z","iopub.status.idle":"2025-08-21T13:33:57.648797Z","shell.execute_reply.started":"2025-08-21T13:33:57.638270Z","shell.execute_reply":"2025-08-21T13:33:57.647502Z"}},"outputs":[],"execution_count":6},{"id":"8dee361c-0162-49df-8dfb-acc9ada50188","cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:57.649651Z","iopub.execute_input":"2025-08-21T13:33:57.650129Z","iopub.status.idle":"2025-08-21T13:33:57.692474Z","shell.execute_reply.started":"2025-08-21T13:33:57.650109Z","shell.execute_reply":"2025-08-21T13:33:57.691724Z"}},"outputs":[],"execution_count":7},{"id":"38617950-e7e3-4810-b396-710e02319e37","cell_type":"code","source":"dataset = FaceOcclusionDataset(DATA_DIR, transform_train)\nloader  = DataLoader(dataset, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:57.693351Z","iopub.execute_input":"2025-08-21T13:33:57.693620Z","iopub.status.idle":"2025-08-21T13:33:57.718906Z","shell.execute_reply.started":"2025-08-21T13:33:57.693587Z","shell.execute_reply":"2025-08-21T13:33:57.718229Z"}},"outputs":[],"execution_count":8},{"id":"0fc4d585-3de0-4f1f-91ed-70e0836dd3fc","cell_type":"markdown","source":"# Define Generator (ResUNet) + PatchGAN Discriminator","metadata":{}},{"id":"2ecd2450-c7f6-4058-9bbf-d97d155a9003","cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, c):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(c, c, 3, 1, 1),\n            nn.BatchNorm2d(c),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(c, c, 3, 1, 1),\n            nn.BatchNorm2d(c)\n        )\n        self.act = nn.ReLU(inplace=True)\n    def forward(self, x): return self.act(self.block(x) + x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:57.719648Z","iopub.execute_input":"2025-08-21T13:33:57.719895Z","iopub.status.idle":"2025-08-21T13:33:57.724352Z","shell.execute_reply.started":"2025-08-21T13:33:57.719868Z","shell.execute_reply":"2025-08-21T13:33:57.723663Z"}},"outputs":[],"execution_count":9},{"id":"713914ca-90d6-4c01-b39a-11376b416878","cell_type":"code","source":"class ResUNetGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        def down(i,o): return nn.Sequential(\n            nn.Conv2d(i,o,4,2,1), nn.BatchNorm2d(o), nn.ReLU(inplace=True))\n        def up(i,o):   return nn.Sequential(\n            nn.ConvTranspose2d(i,o,4,2,1), nn.BatchNorm2d(o), nn.ReLU(inplace=True))\n\n        self.enc1 = down(3,64)     # 256->128\n        self.enc2 = down(64,128)   # 128->64\n        self.enc3 = down(128,256)  # 64 ->32\n        self.res  = ResBlock(256)\n\n        self.dec3 = up(256,128)    # 32->64\n        self.dec2 = up(128,64)     # 64->128\n        self.dec1 = up(64,32)      # 128->256\n\n        # fuse convs for concat skips\n        self.dec3_fuse = nn.Sequential(\n            nn.Conv2d(128+128,128,3,1,1), nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n        self.dec2_fuse = nn.Sequential(\n            nn.Conv2d(64+64,64,3,1,1),   nn.BatchNorm2d(64),  nn.ReLU(inplace=True))\n\n        self.out_rgb  = nn.Conv2d(32, 3, 3, 1, 1)   # ảnh tái tạo\n        self.out_mask = nn.Conv2d(32, 1, 3, 1, 1)   # dự đoán mask\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        e1 = self.enc1(x)                  # [B,64,128,128]\n        e2 = self.enc2(e1)                 # [B,128,64,64]\n        e3 = self.enc3(e2)                 # [B,256,32,32]\n        r  = self.res(e3)\n\n        d3 = self.dec3(r)\n        d3 = self.dec3_fuse(torch.cat([crop_like(e2,d3), crop_like(d3,e2)], dim=1))\n\n        d2 = self.dec2(d3)\n        d2 = self.dec2_fuse(torch.cat([crop_like(e1,d2), crop_like(d2,e1)], dim=1))\n\n        d1 = self.dec1(d2)\n        d1 = crop_like(d1, x)              # align 256x256\n\n        out_img  = self.tanh(self.out_rgb(d1))\n        out_mask = torch.sigmoid(self.out_mask(d1)) # [B,1,H,W]\n        return out_img, out_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:57.726644Z","iopub.execute_input":"2025-08-21T13:33:57.726836Z","iopub.status.idle":"2025-08-21T13:33:57.739680Z","shell.execute_reply.started":"2025-08-21T13:33:57.726824Z","shell.execute_reply":"2025-08-21T13:33:57.739163Z"}},"outputs":[],"execution_count":10},{"id":"896804a5-212b-4881-8123-41a41c4a6cf1","cell_type":"code","source":"\nclass PatchDiscriminator(nn.Module):\n    def __init__(self, in_c=3, ndf=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_c, ndf, 4, 2, 1),            \n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf*2, 4, 2, 1),           \n            nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1),         \n            nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf*4, ndf*8, 4, 1, 1),         \n            nn.BatchNorm2d(ndf*8), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf*8, 1, 4, 1, 1)             \n        )\n    def forward(self, x): return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:57.740447Z","iopub.execute_input":"2025-08-21T13:33:57.740675Z","iopub.status.idle":"2025-08-21T13:33:57.753367Z","shell.execute_reply.started":"2025-08-21T13:33:57.740654Z","shell.execute_reply":"2025-08-21T13:33:57.752775Z"}},"outputs":[],"execution_count":11},{"id":"59e3ba64-b839-4912-b48f-b5b0656ee87f","cell_type":"code","source":"G = ResUNetGenerator().to(DEVICE)\nD = PatchDiscriminator().to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:57.754207Z","iopub.execute_input":"2025-08-21T13:33:57.754694Z","iopub.status.idle":"2025-08-21T13:33:58.031515Z","shell.execute_reply.started":"2025-08-21T13:33:57.754676Z","shell.execute_reply":"2025-08-21T13:33:58.030730Z"}},"outputs":[],"execution_count":12},{"id":"853bbf7b-f43a-4168-b8a1-71cee43f04ee","cell_type":"markdown","source":"# Loss","metadata":{}},{"id":"08b94d24-e263-46a4-84b9-21ce90e78009","cell_type":"code","source":"bce_logits = nn.BCEWithLogitsLoss()\nl1 = nn.L1Loss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:58.032380Z","iopub.execute_input":"2025-08-21T13:33:58.032656Z","iopub.status.idle":"2025-08-21T13:33:58.036511Z","shell.execute_reply.started":"2025-08-21T13:33:58.032633Z","shell.execute_reply":"2025-08-21T13:33:58.035854Z"}},"outputs":[],"execution_count":13},{"id":"da51686e-75bd-4a4c-8693-94bf237717f6","cell_type":"code","source":"def laplace(x):\n    k = torch.tensor([[0,1,0],[1,-4,1],[0,1,0]], dtype=x.dtype, device=x.device).view(1,1,3,3)\n    k = k.repeat(3,1,1,1)\n    return F.conv2d(x, k, padding=1, groups=3)\n\ndef edge_loss(fake, real):\n    return F.l1_loss(laplace(fake), laplace(real))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:58.037333Z","iopub.execute_input":"2025-08-21T13:33:58.037588Z","iopub.status.idle":"2025-08-21T13:33:58.048020Z","shell.execute_reply.started":"2025-08-21T13:33:58.037571Z","shell.execute_reply":"2025-08-21T13:33:58.047322Z"}},"outputs":[],"execution_count":14},{"id":"f833d9f8-7414-4ebe-9319-9df229ddab79","cell_type":"code","source":"USE_PERCEPTUAL = True\nperc_lambda = 0.1\ntry:\n    from torchvision.models import vgg19, VGG19_Weights\n    _vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features.to(DEVICE).eval()\n    for p in _vgg.parameters(): p.requires_grad = False\n    VGG_READY = True\n    vgg_layers = [3, 8, 17]\n    def perceptual_feats(x01):  # x in [0,1]\n        feats = []\n        z = x01\n        for i, layer in enumerate(_vgg):\n            z = layer(z)\n            if i in vgg_layers: feats.append(z)\n        return feats\n    print(\"Perceptual loss ON\")\nexcept Exception as e:\n    VGG_READY = False\n    USE_PERCEPTUAL = False\n    print(\"Perceptual loss OFF:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:33:58.048830Z","iopub.execute_input":"2025-08-21T13:33:58.049335Z","iopub.status.idle":"2025-08-21T13:34:02.812739Z","shell.execute_reply.started":"2025-08-21T13:33:58.049316Z","shell.execute_reply":"2025-08-21T13:34:02.812087Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 206MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Perceptual loss ON\n","output_type":"stream"}],"execution_count":15},{"id":"d5cc0500-8dec-468a-8e44-c8a8b5587c62","cell_type":"code","source":"opt_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5,0.999))\nopt_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5,0.999))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:34:02.813485Z","iopub.execute_input":"2025-08-21T13:34:02.813747Z","iopub.status.idle":"2025-08-21T13:34:02.818218Z","shell.execute_reply.started":"2025-08-21T13:34:02.813721Z","shell.execute_reply":"2025-08-21T13:34:02.817620Z"}},"outputs":[],"execution_count":16},{"id":"602b0b34-0f3f-434e-803a-7782f8506455","cell_type":"code","source":"lambda_rec_in  = 10.0\nlambda_rec_out = 1.0\nlambda_edge    = 0.1\nlambda_mask    = 1.0\nlambda_change  = 2.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:34:02.818984Z","iopub.execute_input":"2025-08-21T13:34:02.819256Z","iopub.status.idle":"2025-08-21T13:34:02.830666Z","shell.execute_reply.started":"2025-08-21T13:34:02.819182Z","shell.execute_reply":"2025-08-21T13:34:02.829932Z"}},"outputs":[],"execution_count":17},{"id":"cb4753ca-c64c-4420-b89b-77ee7125e9ca","cell_type":"markdown","source":"# Training","metadata":{}},{"id":"ef389a0a-27dc-4912-9846-58d77a1ca12c","cell_type":"code","source":"SAVE_PATH = \"/kaggle/working/checkpoints\"\nos.makedirs(SAVE_PATH, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:34:02.831302Z","iopub.execute_input":"2025-08-21T13:34:02.831516Z","iopub.status.idle":"2025-08-21T13:34:02.840878Z","shell.execute_reply.started":"2025-08-21T13:34:02.831492Z","shell.execute_reply":"2025-08-21T13:34:02.840170Z"}},"outputs":[],"execution_count":18},{"id":"291fb82d-62a7-4866-ba60-8d4670ff6e1c","cell_type":"code","source":"for epoch in range(EPOCHS):\n    for i, (damaged, real, mask) in enumerate(loader):\n        damaged = damaged.to(DEVICE, non_blocking=True)\n        real    = real.to(DEVICE, non_blocking=True)\n        mask    = mask.to(DEVICE, non_blocking=True)     # [B,1,H,W] in {0,1}\n\n        # --------- Train D ---------\n        with torch.no_grad():\n            fake_det, _ = G(damaged)\n            fake_det = crop_like(fake_det, real)\n\n        loss_D = bce_logits(D(real), torch.ones_like(D(real))) + \\\n                 bce_logits(D(fake_det), torch.zeros_like(D(fake_det)))\n        opt_D.zero_grad(set_to_none=True); loss_D.backward(); opt_D.step()\n\n        # --------- Train G ---------\n        fake, mask_pred = G(damaged)\n        fake      = crop_like(fake, real)\n        mask_pred = crop_like(mask_pred, mask)\n\n        adv_loss = bce_logits(D(fake), torch.ones_like(D(fake)))\n\n        # tái tạo có trọng số\n        rec_in  = (mask  * (fake - real).abs()).mean()\n        rec_out = ((1.0-mask) * (fake - real).abs()).mean()\n\n        diff_in = (mask * (fake - damaged).abs()).mean()\n        loss_change = -diff_in   # maximize diff_in\n\n        e_loss  = edge_loss(fake, real)\n        m_loss  = F.binary_cross_entropy(mask_pred, mask)\n\n        loss_G = adv_loss \\\n                 + lambda_rec_in*rec_in + lambda_rec_out*rec_out \\\n                 + lambda_edge*e_loss + lambda_mask*m_loss \\\n                 + lambda_change*loss_change\n\n        if USE_PERCEPTUAL and VGG_READY:\n            f_feats = perceptual_feats(denorm(fake))\n            r_feats = perceptual_feats(denorm(real))\n            p_loss  = sum(F.l1_loss(f, r) for f, r in zip(f_feats, r_feats))\n            loss_G  = loss_G + perc_lambda * p_loss\n\n        opt_G.zero_grad(set_to_none=True); loss_G.backward(); opt_G.step()\n\n        if i % 100 == 0:\n            print(f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(loader)}] \"\n                  f\"D:{loss_D.item():.3f} | G:{loss_G.item():.3f} | \"\n                  f\"adv:{adv_loss.item():.3f} rec_in:{rec_in.item():.3f} \"\n                  f\"rec_out:{rec_out.item():.3f} edge:{e_loss.item():.3f} \"\n                  f\"mask:{m_loss.item():.3f} diff_in:{diff_in.item():.3f}\")\n            grid = torch.cat([damaged[:4], fake[:4], real[:4]], dim=0)\n            save_image(denorm(grid), f\"{SAVE_PATH}/sample_{epoch}_{i}.png\", nrow=4)\n\n    torch.save(G.state_dict(), f\"{SAVE_PATH}/G_epoch{epoch}.pth\")\n    torch.save(D.state_dict(), f\"{SAVE_PATH}/D_epoch{epoch}.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:34:02.841749Z","iopub.execute_input":"2025-08-21T13:34:02.842012Z","execution_failed":"2025-08-21T13:34:53.219Z"}},"outputs":[{"name":"stdout","text":"[Epoch 0/30] [Batch 0/313] D:1.415 | G:2.918 | adv:0.919 rec_in:0.074 rec_out:0.437 edge:1.212 mask:0.701 diff_in:0.075\n","output_type":"stream"}],"execution_count":null},{"id":"6e08108f-4f70-49f1-b26b-896fd67fee65","cell_type":"markdown","source":"# LIST & LOAD LATEST CHECKPOINT","metadata":{}},{"id":"c23e9a8a-2f71-4277-97cd-7161a0980eb3","cell_type":"code","source":"def extract_epoch(path):\n    m = re.search(r'_(\\d+)\\.pth$', path)\n    return int(m.group(1)) if m else -1\n\ng_files = sorted(glob.glob(f\"{SAVE_PATH}/G_epoch*.pth\"), key=extract_epoch)\nif g_files:\n    latest = g_files[-1]\n    print(\"Load:\", latest)\n    G.load_state_dict(torch.load(latest, map_location=DEVICE))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:34:53.219Z"}},"outputs":[],"execution_count":null},{"id":"06570921-f764-4a82-b1a3-35817137cfff","cell_type":"markdown","source":"# INFERENCE on a real test image","metadata":{}},{"id":"186a91b2-d2d5-4974-8960-9872f4b6c93f","cell_type":"code","source":"os.makedirs(\"/kaggle/working/output\", exist_ok=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:34:53.220Z"}},"outputs":[],"execution_count":null},{"id":"7ad6b7d6-f06b-4d6f-bc13-07fda1792360","cell_type":"code","source":"def reconstruct_image(model, input_path, out_path):\n    model.eval()\n    img = Image.open(input_path).convert(\"RGB\")\n    tfm = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ])\n    t = tfm(img).unsqueeze(0).to(DEVICE)\n    with torch.no_grad():\n        out_img, _ = model(t)\n        out_img = crop_like(out_img, t)\n    save_image(denorm(out_img), out_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:34:53.220Z"}},"outputs":[],"execution_count":null},{"id":"4da4f71e-4f9c-4e6b-bbd5-b0b84743c256","cell_type":"code","source":"test_inp = '/kaggle/input/testdata/test.png'  \ntest_out = '/kaggle/working/output/reconstructed.png'\nreconstruct_image(G, test_inp, test_out)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:34:53.220Z"}},"outputs":[],"execution_count":null},{"id":"e4d17774-1813-43d5-acde-a4a9dca90653","cell_type":"markdown","source":"\n# SHOW INPUT vs OUTPUT","metadata":{}},{"id":"0d4f2485-194f-4cde-a83b-0970083f1f20","cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(10,5))\nax[0].imshow(Image.open(test_inp));  ax[0].set_title(\"Input\");  ax[0].axis(\"off\")\nax[1].imshow(Image.open(test_out));  ax[1].set_title(\"Output\"); ax[1].axis(\"off\")\nplt.tight_layout(); plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:34:53.220Z"}},"outputs":[],"execution_count":null}]}