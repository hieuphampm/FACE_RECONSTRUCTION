{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12817559,"sourceType":"datasetVersion","datasetId":8087825},{"sourceId":12821797,"sourceType":"datasetVersion","datasetId":8108119},{"sourceId":12821835,"sourceType":"datasetVersion","datasetId":8108149},{"sourceId":12821850,"sourceType":"datasetVersion","datasetId":8108160},{"sourceId":12825538,"sourceType":"datasetVersion","datasetId":8110818},{"sourceId":12862072,"sourceType":"datasetVersion","datasetId":8105308}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f89d8bf8-9f77-4a19-988d-e6004d951feb","cell_type":"markdown","source":"# Lib","metadata":{}},{"id":"3bbee187-2012-4aa8-bd65-be78f7d988f5","cell_type":"code","source":"!pip -q install --no-input opencv-python-headless==4.10.0.84 tqdm==4.66.4 imageio==2.35.1\n!pip -q install --no-input simple-lama-inpainting==0.1.2\n!pip -q install --no-input gfpgan==1.3.8 basicsr==1.4.2 facexlib==0.3.0 realesrgan==0.3.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:16:46.920447Z","iopub.execute_input":"2025-08-25T11:16:46.920730Z","iopub.status.idle":"2025-08-25T11:18:16.618966Z","shell.execute_reply.started":"2025-08-25T11:16:46.920707Z","shell.execute_reply":"2025-08-25T11:18:16.618213Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.4/315.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nscikit-image 0.25.2 requires pillow>=10.1, but you have pillow 9.5.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.6/299.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsimple-lama-inpainting 0.1.2 requires pillow<10.0.0,>=9.5.0, but you have pillow 11.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"dcf50731-d0db-41f2-a086-1104d054cfd1","cell_type":"code","source":"import os, glob, cv2, numpy as np\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:18:16.620636Z","iopub.execute_input":"2025-08-25T11:18:16.620933Z","iopub.status.idle":"2025-08-25T11:18:16.666689Z","shell.execute_reply.started":"2025-08-25T11:18:16.620896Z","shell.execute_reply":"2025-08-25T11:18:16.665931Z"}},"outputs":[],"execution_count":2},{"id":"256c00ec-dbe5-473b-8edf-09b2bfb13cb9","cell_type":"code","source":"paths = [\n    '/kaggle/input/flickr-faces-hq/00000',\n    '/kaggle/input/flickr-faces-hq-2/01000',\n    '/kaggle/input/flickr-faces-hq-3/02000',\n    '/kaggle/input/flickr-faces-hq-4/03000',\n    '/kaggle/input/flickr-faces-hq-5/04000'\n]\nDATA_DIR  = '/kaggle/working/all_images'\nSIZE      = 256\nLIMIT     = None\nTEST_DIR  = '/kaggle/input/testdata'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:18:16.667330Z","iopub.execute_input":"2025-08-25T11:18:16.667516Z","iopub.status.idle":"2025-08-25T11:18:16.671412Z","shell.execute_reply.started":"2025-08-25T11:18:16.667499Z","shell.execute_reply":"2025-08-25T11:18:16.670779Z"}},"outputs":[],"execution_count":3},{"id":"bb9daabd-f5eb-43b4-a105-fe1769576101","cell_type":"code","source":"IMG_DIR  = os.path.join(DATA_DIR, 'images')\nOUT_DIR  = '/kaggle/working/outputs'\nCKPT_DIR = '/kaggle/working/checkpoints'\nfor d in [DATA_DIR, IMG_DIR, OUT_DIR, CKPT_DIR]:\n    os.makedirs(d, exist_ok=True)\nprint(\"READY: DATA_DIR=\", DATA_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:18:16.672788Z","iopub.execute_input":"2025-08-25T11:18:16.672992Z","iopub.status.idle":"2025-08-25T11:18:16.683394Z","shell.execute_reply.started":"2025-08-25T11:18:16.672975Z","shell.execute_reply":"2025-08-25T11:18:16.682849Z"}},"outputs":[{"name":"stdout","text":"READY: DATA_DIR= /kaggle/working/all_images\n","output_type":"stream"}],"execution_count":4},{"id":"47dc613a-e942-483d-9fee-00daafa3fe8c","cell_type":"markdown","source":"# Chuẩn bị dữ liệu: gom ảnh","metadata":{}},{"id":"2c13d5d7-a033-401c-9f57-7087700e6b98","cell_type":"code","source":"import cv2, traceback\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:18:16.684095Z","iopub.execute_input":"2025-08-25T11:18:16.684825Z","iopub.status.idle":"2025-08-25T11:18:16.807450Z","shell.execute_reply.started":"2025-08-25T11:18:16.684806Z","shell.execute_reply":"2025-08-25T11:18:16.806731Z"}},"outputs":[],"execution_count":5},{"id":"f9830307-058d-47fd-860a-48d14ae33a9c","cell_type":"code","source":"def is_image_file(p: Path):\n    return p.suffix.lower() in {'.jpg','.jpeg','.png','.bmp','.webp'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:18:16.808246Z","iopub.execute_input":"2025-08-25T11:18:16.808485Z","iopub.status.idle":"2025-08-25T11:18:16.812352Z","shell.execute_reply.started":"2025-08-25T11:18:16.808461Z","shell.execute_reply":"2025-08-25T11:18:16.811603Z"}},"outputs":[],"execution_count":6},{"id":"7c02a6aa-81d8-4440-8d84-ca0bae0374e0","cell_type":"code","source":"def gather_images(src_paths, img_dir, limit=None, resize=None):\n    pths = []\n    for sp in src_paths:\n        p = Path(sp)\n        if not p.exists():\n            print(f\"[WARN] Not found: {sp}\")\n            continue\n        for f in p.iterdir():\n            if is_image_file(f):\n                pths.append(f)\n    if limit is not None:\n        pths = pths[:int(limit)]\n    print(f\"Found {len(pths)} images.\")\n\n    count = 0\n    for f in tqdm(pths, desc=\"Copying\"):\n        try:\n            img = cv2.imread(str(f))\n            if img is None: continue\n            if resize is not None:\n                img = cv2.resize(img, resize, interpolation=cv2.INTER_AREA)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            out_path = Path(img_dir)/f\"{count:06d}.png\"\n            cv2.imwrite(str(out_path), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n            count += 1\n        except Exception:\n            print(\"Skip:\", f, \"\\n\", traceback.format_exc())\n    print(\"Gathered:\", count)\n    return count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:18:16.812983Z","iopub.execute_input":"2025-08-25T11:18:16.813138Z","iopub.status.idle":"2025-08-25T11:18:16.824086Z","shell.execute_reply.started":"2025-08-25T11:18:16.813125Z","shell.execute_reply":"2025-08-25T11:18:16.823434Z"}},"outputs":[],"execution_count":7},{"id":"2d2f88a2-c74c-4cb7-88a2-77ae31fdbdd1","cell_type":"code","source":"N = gather_images(paths, IMG_DIR, limit=LIMIT, resize=None)\nprint(\"Total images collected:\", N)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:18:16.824820Z","iopub.execute_input":"2025-08-25T11:18:16.825071Z","iopub.status.idle":"2025-08-25T11:27:46.371301Z","shell.execute_reply.started":"2025-08-25T11:18:16.825045Z","shell.execute_reply":"2025-08-25T11:27:46.370540Z"}},"outputs":[{"name":"stdout","text":"Found 5000 images.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Copying:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587a1079862741608d821924e270625f"}},"metadata":{}},{"name":"stdout","text":"Gathered: 5000\nTotal images collected: 5000\n","output_type":"stream"}],"execution_count":8},{"id":"8c93ab0e-4c16-407b-8b46-b37b49c821f0","cell_type":"markdown","source":"# Dataset + DataLoader","metadata":{}},{"id":"bb69940e-8a44-4f50-bb50-ac9315e59e57","cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np, cv2, glob, os\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:46.372211Z","iopub.execute_input":"2025-08-25T11:27:46.372511Z","iopub.status.idle":"2025-08-25T11:27:49.343731Z","shell.execute_reply.started":"2025-08-25T11:27:46.372455Z","shell.execute_reply":"2025-08-25T11:27:49.342996Z"}},"outputs":[],"execution_count":9},{"id":"27c711c5-48d0-4452-b42c-b69f4d990e5f","cell_type":"code","source":"def to_tensor_img(img_np):\n    return torch.from_numpy((img_np.astype(np.float32)/127.5 - 1.0)).permute(2,0,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.346068Z","iopub.execute_input":"2025-08-25T11:27:49.346329Z","iopub.status.idle":"2025-08-25T11:27:49.350094Z","shell.execute_reply.started":"2025-08-25T11:27:49.346314Z","shell.execute_reply":"2025-08-25T11:27:49.349427Z"}},"outputs":[],"execution_count":10},{"id":"51fe3153-dfed-4328-8dc3-e890625701ee","cell_type":"code","source":"def resize_longest_maxsize_and_pad(img, size=256):\n    h, w = img.shape[:2]\n    if h == 0 or w == 0:\n        return np.zeros((size, size, 3), np.uint8)\n    scale = size / max(h, w)\n    new_w = max(1, int(round(w * scale)))\n    new_h = max(1, int(round(h * scale)))\n    interp = cv2.INTER_AREA if scale < 1 else cv2.INTER_CUBIC\n    resized = cv2.resize(img, (new_w, new_h), interpolation=interp)\n    top = (size - new_h) // 2; bottom = size - new_h - top\n    left = (size - new_w) // 2; right  = size - new_w - left\n    return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_REFLECT_101)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.350783Z","iopub.execute_input":"2025-08-25T11:27:49.351082Z","iopub.status.idle":"2025-08-25T11:27:49.362129Z","shell.execute_reply.started":"2025-08-25T11:27:49.351062Z","shell.execute_reply":"2025-08-25T11:27:49.361382Z"}},"outputs":[],"execution_count":11},{"id":"525a0017-372a-4d6d-9a68-620cc7d91e88","cell_type":"code","source":"def random_brush_mask(h, w):\n    m = np.zeros((h, w), np.uint8)\n    n = np.random.randint(1, 4)\n\n    def safe_size(lo, hi):\n        hi = max(hi, lo + 1)\n        return np.random.randint(lo, hi)\n\n    for _ in range(n):\n        typ = np.random.choice([\"ellipse\",\"rect\",\"stroke\",\"poly\"])\n        if typ == \"rect\":\n            rw = safe_size(max(8, w//16), max(9, w//3))\n            rh = safe_size(max(8, h//16), max(9, h//3))\n            x1 = 0 if w-rw <= 1 else np.random.randint(0, w - rw)\n            y1 = 0 if h-rh <= 1 else np.random.randint(0, h - rh)\n            cv2.rectangle(m, (x1,y1), (x1+rw, y1+rh), 255, -1)\n        elif typ == \"ellipse\":\n            ax = safe_size(max(6, w//16), max(7, w//4))\n            ay = safe_size(max(6, h//16), max(7, h//4))\n            cx = (w//2) if w-2*ax<=1 else np.random.randint(ax, w-ax)\n            cy = (h//2) if h-2*ay<=1 else np.random.randint(ay, h-ay)\n            ang = np.random.randint(0, 180)\n            cv2.ellipse(m, (cx,cy), (ax,ay), int(ang), 0, 360, 255, -1)\n        elif typ == \"poly\":\n            pts = np.stack([np.random.randint(0,w, size=6), np.random.randint(0,h, size=6)], axis=1)\n            cv2.fillPoly(m, [pts.astype(np.int32)], 255)\n        else:\n            steps = np.random.randint(6, 12)\n            pts = [(np.random.randint(0,w), np.random.randint(0,h)) for _ in range(steps)]\n            rmax = max(4, min(h, w)//12)\n            for i in range(len(pts)-1):\n                r = int(np.random.randint(3, rmax))\n                cv2.line(m, pts[i], pts[i+1], 255, r)\n    return m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.362983Z","iopub.execute_input":"2025-08-25T11:27:49.363229Z","iopub.status.idle":"2025-08-25T11:27:49.374282Z","shell.execute_reply.started":"2025-08-25T11:27:49.363208Z","shell.execute_reply":"2025-08-25T11:27:49.373601Z"}},"outputs":[],"execution_count":12},{"id":"eb319c0a-2ca6-4fc1-93a2-bc41c9402073","cell_type":"code","source":"def occ_blend(img, mask):\n    occ = img.copy()\n    if np.random.rand() < 0.5:\n        color = np.uint8(np.random.randint(0,256, size=(1,1,3)))\n        occ[mask>0] = (0.6*occ[mask>0] + 0.4*color).astype(np.uint8)\n    else:\n        noise = np.clip(np.random.randn(*occ.shape)*30 + 128, 0, 255).astype(np.uint8)\n        occ[mask>0] = noise[mask>0]\n    return occ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.375023Z","iopub.execute_input":"2025-08-25T11:27:49.375214Z","iopub.status.idle":"2025-08-25T11:27:49.387083Z","shell.execute_reply.started":"2025-08-25T11:27:49.375198Z","shell.execute_reply":"2025-08-25T11:27:49.386447Z"}},"outputs":[],"execution_count":13},{"id":"4fd6ec6e-4644-460e-99b0-01fe72fc0976","cell_type":"code","source":"class FaceOccDataset(Dataset):\n    def __init__(self, img_dir, size=256):\n        self.paths = sorted([p for p in Path(img_dir).glob(\"*.png\")])\n        self.size = size\n    def __len__(self): return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        img = cv2.imread(str(p))\n        if img is None:\n            img = np.zeros((self.size, self.size, 3), np.uint8)\n        else:\n            img = img[:, :, ::-1]\n        img = resize_longest_maxsize_and_pad(img, size=self.size)\n        m = random_brush_mask(self.size, self.size)\n        occ = occ_blend(img, m)\n        return {\n            'img':  to_tensor_img(img),\n            'occ':  to_tensor_img(occ),\n            'mask': torch.from_numpy((m>127).astype(np.float32))[None],\n            'path': str(p)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.387914Z","iopub.execute_input":"2025-08-25T11:27:49.388667Z","iopub.status.idle":"2025-08-25T11:27:49.398341Z","shell.execute_reply.started":"2025-08-25T11:27:49.388641Z","shell.execute_reply":"2025-08-25T11:27:49.397687Z"}},"outputs":[],"execution_count":14},{"id":"eda87ac2-e34d-4ad1-b89e-248d3c4de8a6","cell_type":"code","source":"BATCH_SIZE = 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.398963Z","iopub.execute_input":"2025-08-25T11:27:49.399176Z","iopub.status.idle":"2025-08-25T11:27:49.410072Z","shell.execute_reply.started":"2025-08-25T11:27:49.399161Z","shell.execute_reply":"2025-08-25T11:27:49.409438Z"}},"outputs":[],"execution_count":15},{"id":"eb99c3bf-d92a-47de-8cb6-e5eda6dc3f0d","cell_type":"code","source":"train_ds = FaceOccDataset(IMG_DIR, size=SIZE)\nval_ds   = FaceOccDataset(IMG_DIR, size=SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.410711Z","iopub.execute_input":"2025-08-25T11:27:49.410970Z","iopub.status.idle":"2025-08-25T11:27:49.479318Z","shell.execute_reply.started":"2025-08-25T11:27:49.410954Z","shell.execute_reply":"2025-08-25T11:27:49.478737Z"}},"outputs":[],"execution_count":16},{"id":"f12531ad-025e-4a08-b7cf-88b444080e28","cell_type":"code","source":"train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\nprint(\"Train/Val:\", len(train_ds), len(val_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.480034Z","iopub.execute_input":"2025-08-25T11:27:49.480234Z","iopub.status.idle":"2025-08-25T11:27:49.485559Z","shell.execute_reply.started":"2025-08-25T11:27:49.480210Z","shell.execute_reply":"2025-08-25T11:27:49.484826Z"}},"outputs":[{"name":"stdout","text":"Train/Val: 5000 5000\n","output_type":"stream"}],"execution_count":17},{"id":"14dc0b58-2abc-4cb1-8fa8-8fa8340a10ea","cell_type":"markdown","source":"# MaskNet (UNet nhỏ) + Dice Loss + EMA","metadata":{}},{"id":"c25a51c6-9b9e-491a-b006-a5844b8be531","cell_type":"code","source":"import torch, torch.nn as nn, torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.486334Z","iopub.execute_input":"2025-08-25T11:27:49.486573Z","iopub.status.idle":"2025-08-25T11:27:49.495081Z","shell.execute_reply.started":"2025-08-25T11:27:49.486550Z","shell.execute_reply":"2025-08-25T11:27:49.494431Z"}},"outputs":[],"execution_count":18},{"id":"2fbcddbb-765f-4a5e-98d0-3fc72bd665b7","cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self,c_in,c_out):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(c_in,c_out,3,1,1), nn.BatchNorm2d(c_out), nn.ReLU(True),\n            nn.Conv2d(c_out,c_out,3,1,1), nn.BatchNorm2d(c_out), nn.ReLU(True),\n        )\n    def forward(self,x): return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.495731Z","iopub.execute_input":"2025-08-25T11:27:49.495996Z","iopub.status.idle":"2025-08-25T11:27:49.506088Z","shell.execute_reply.started":"2025-08-25T11:27:49.495973Z","shell.execute_reply":"2025-08-25T11:27:49.505304Z"}},"outputs":[],"execution_count":19},{"id":"fd8ab03a-4c67-4823-82d9-266f4fc202e5","cell_type":"code","source":"class UNetSmall(nn.Module):\n    def __init__(self, in_ch=3, out_ch=1, ch=32):\n        super().__init__()\n        self.d1=DoubleConv(in_ch, ch); self.p1=nn.MaxPool2d(2)\n        self.d2=DoubleConv(ch, ch*2);  self.p2=nn.MaxPool2d(2)\n        self.d3=DoubleConv(ch*2, ch*4);self.p3=nn.MaxPool2d(2)\n        self.b =DoubleConv(ch*4, ch*8)\n        self.u3=nn.ConvTranspose2d(ch*8, ch*4, 2,2); self.h3=DoubleConv(ch*8, ch*4)\n        self.u2=nn.ConvTranspose2d(ch*4, ch*2, 2,2); self.h2=DoubleConv(ch*4, ch*2)\n        self.u1=nn.ConvTranspose2d(ch*2, ch,   2,2); self.h1=DoubleConv(ch*2, ch)\n        self.out=nn.Conv2d(ch, out_ch, 1)\n    def forward(self,x):\n        x1=self.d1(x); x1p=self.p1(x1)\n        x2=self.d2(x1p); x2p=self.p2(x2)\n        x3=self.d3(x2p); x3p=self.p3(x3)\n        xb=self.b(x3p)\n        x = self.u3(xb); x=self.h3(torch.cat([x, x3],1))\n        x = self.u2(x);  x=self.h2(torch.cat([x, x2],1))\n        x = self.u1(x);  x=self.h1(torch.cat([x, x1],1))\n        return self.out(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.506921Z","iopub.execute_input":"2025-08-25T11:27:49.507267Z","iopub.status.idle":"2025-08-25T11:27:49.516992Z","shell.execute_reply.started":"2025-08-25T11:27:49.507243Z","shell.execute_reply":"2025-08-25T11:27:49.516368Z"}},"outputs":[],"execution_count":20},{"id":"d7d05ab8-6e4b-4efc-8cb6-86c88f291c40","cell_type":"code","source":"class EMA:\n    def __init__(self, model, decay=0.999):\n        self.decay = decay\n        self.param_shadow = {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n        self.buff_shadow  = {n: b.detach().clone() for n,b in model.named_buffers() if torch.is_floating_point(b)}\n    @torch.no_grad()\n    def update(self, model):\n        for n,p in model.named_parameters():\n            if not p.requires_grad: continue\n            self.param_shadow[n].mul_(self.decay).add_(p.detach(), alpha=1-self.decay)\n        for n,b in model.named_buffers():\n            if not torch.is_floating_point(b): continue\n            self.buff_shadow[n].mul_(self.decay).add_(b.detach(), alpha=1-self.decay)\n    @torch.no_grad()\n    def copy_to(self, model):\n        for n,p in model.named_parameters():\n            if n in self.param_shadow:\n                p.data.copy_(self.param_shadow[n])\n        for n,b in model.named_buffers():\n            if n in self.buff_shadow and torch.is_floating_point(b):\n                b.data.copy_(self.buff_shadow[n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.517737Z","iopub.execute_input":"2025-08-25T11:27:49.518009Z","iopub.status.idle":"2025-08-25T11:27:49.530317Z","shell.execute_reply.started":"2025-08-25T11:27:49.517980Z","shell.execute_reply":"2025-08-25T11:27:49.529635Z"}},"outputs":[],"execution_count":21},{"id":"e315f155-599b-4d06-bdea-a2e224d574d1","cell_type":"code","source":"def dice_loss(pred, target, eps=1e-6):\n    num = 2*(pred*target).sum(dim=(2,3))\n    den = pred.sum(dim=(2,3)) + target.sum(dim=(2,3)) + eps\n    return (1-(num/den)).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.531108Z","iopub.execute_input":"2025-08-25T11:27:49.531367Z","iopub.status.idle":"2025-08-25T11:27:49.543307Z","shell.execute_reply.started":"2025-08-25T11:27:49.531343Z","shell.execute_reply":"2025-08-25T11:27:49.542587Z"}},"outputs":[],"execution_count":22},{"id":"570955ed-efc2-4717-8dc5-2694ed2dc782","cell_type":"markdown","source":"# Train MaskNet (AMP + checkpoint)","metadata":{}},{"id":"f8519fa0-da2b-4ffe-a84d-2e24ec622eff","cell_type":"code","source":"import torch, os\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.544041Z","iopub.execute_input":"2025-08-25T11:27:49.544241Z","iopub.status.idle":"2025-08-25T11:27:49.553220Z","shell.execute_reply.started":"2025-08-25T11:27:49.544224Z","shell.execute_reply":"2025-08-25T11:27:49.552638Z"}},"outputs":[],"execution_count":23},{"id":"554526d0-d356-46cf-ab31-6a262e06ef82","cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet = UNetSmall().to(device)\nopt = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-4)\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\nema = EMA(net, decay=0.999)\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:49.553967Z","iopub.execute_input":"2025-08-25T11:27:49.554227Z","iopub.status.idle":"2025-08-25T11:27:52.650646Z","shell.execute_reply.started":"2025-08-25T11:27:49.554205Z","shell.execute_reply":"2025-08-25T11:27:52.649912Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1977812794.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","output_type":"stream"}],"execution_count":24},{"id":"b167a6d9-0058-4078-a3c1-d899d51c2062","cell_type":"code","source":"EPOCHS= 20\nbest_iou= 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:52.651328Z","iopub.execute_input":"2025-08-25T11:27:52.651665Z","iopub.status.idle":"2025-08-25T11:27:52.656185Z","shell.execute_reply.started":"2025-08-25T11:27:52.651647Z","shell.execute_reply":"2025-08-25T11:27:52.655390Z"}},"outputs":[],"execution_count":25},{"id":"c4de0658-2537-488b-a7a7-73c0ebcc4a89","cell_type":"code","source":"def batch_iou(p, t, th=0.5):\n    pbin = (p>th).float()\n    inter = (pbin*t).sum(dim=(2,3))\n    union = (pbin + t - pbin*t).sum(dim=(2,3)) + 1e-6\n    return (inter/union).mean().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:52.656956Z","iopub.execute_input":"2025-08-25T11:27:52.657233Z","iopub.status.idle":"2025-08-25T11:27:52.679646Z","shell.execute_reply.started":"2025-08-25T11:27:52.657210Z","shell.execute_reply":"2025-08-25T11:27:52.678944Z"}},"outputs":[],"execution_count":26},{"id":"1b41acea-cd1e-43dd-9432-a1d75d372ac6","cell_type":"code","source":"for ep in range(1, EPOCHS+1):\n    net.train()\n    pbar = tqdm(train_loader, desc=f\"Train Ep{ep}\")\n    for batch in pbar:\n        occ = batch['occ'].to(device)\n        m   = batch['mask'].to(device)\n        opt.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n            logits = net((occ+1)/2)\n            bce    = torch.nn.functional.binary_cross_entropy_with_logits(logits, m)\n            pm     = torch.sigmoid(logits)\n            dsc    = dice_loss(pm, m)\n            loss   = bce + dsc\n        scaler.scale(loss).backward()\n        scaler.step(opt); scaler.update()\n        ema.update(net)\n        pbar.set_postfix(loss=float(loss.detach().cpu()))\n    # Eval\n    net.eval(); miou=0.0; n=0\n    with torch.no_grad():\n        for batch in val_loader:\n            occ = batch['occ'].to(device); m = batch['mask'].to(device)\n            pm = torch.sigmoid(net((occ+1)/2))\n            miou += batch_iou(pm, m); n+=1\n    miou /= max(n,1)\n    print(f\"[Epoch {ep}] mIoU={miou:.4f}\")\n    if miou > best_iou:\n        best_iou = miou\n        os.makedirs(CKPT_DIR, exist_ok=True)\n        torch.save(net.state_dict(), os.path.join(CKPT_DIR, 'masknet_best.pth'))\n        ema_model = UNetSmall().to(device); ema.copy_to(ema_model)\n        torch.save(ema_model.state_dict(), os.path.join(CKPT_DIR, 'masknet_best_ema.pth'))\n        del ema_model\n        print(\"Saved best to\", CKPT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T11:27:52.680327Z","iopub.execute_input":"2025-08-25T11:27:52.680504Z","execution_failed":"2025-08-25T12:53:13.696Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Train Ep1:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac58fb350711445e98e30280f1030818"}},"metadata":{}},{"name":"stdout","text":"[Epoch 1] mIoU=0.6700\nSaved best to /kaggle/working/checkpoints\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep2:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e410e31353641e38b6f8099443b7571"}},"metadata":{}},{"name":"stdout","text":"[Epoch 2] mIoU=0.6983\nSaved best to /kaggle/working/checkpoints\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep3:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f708653f48e475d90452b099d8255a2"}},"metadata":{}},{"name":"stdout","text":"[Epoch 3] mIoU=0.7208\nSaved best to /kaggle/working/checkpoints\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep4:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae928e6e6b4d4ba19fcd0cdb4f980506"}},"metadata":{}},{"name":"stdout","text":"[Epoch 4] mIoU=0.7000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep5:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f870307761c4841bdc4148e00ab22cd"}},"metadata":{}},{"name":"stdout","text":"[Epoch 5] mIoU=0.6925\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep6:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c22e7d60edb742e4b3f43a7a4bc938f0"}},"metadata":{}},{"name":"stdout","text":"[Epoch 6] mIoU=0.9070\nSaved best to /kaggle/working/checkpoints\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep7:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e6bcc76287f47c1bfe7713e9e76262a"}},"metadata":{}},{"name":"stdout","text":"[Epoch 7] mIoU=0.9151\nSaved best to /kaggle/working/checkpoints\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep8:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e5488f44c594d55abc9430e9598879b"}},"metadata":{}},{"name":"stdout","text":"[Epoch 8] mIoU=0.9441\nSaved best to /kaggle/working/checkpoints\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Ep9:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad3dbbf69444c5e9d0c7c554929d3dc"}},"metadata":{}}],"execution_count":null},{"id":"ecf8ef08-7e4c-423d-84e1-d1a2dfbd2e83","cell_type":"markdown","source":"# Inference: MaskNet → LaMa → GFPGAN","metadata":{}},{"id":"8849afcd-4e59-4067-81c5-198276492c30","cell_type":"code","source":"import os, cv2, urllib.request, numpy as np, types, sys, torchvision, torch\nfrom pathlib import Path\nfrom PIL import Image\nimport torchvision.transforms.functional as F_tvF","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"7b349d51-db47-4c5a-b8d2-cf8c3c448305","cell_type":"code","source":"ft = types.ModuleType(\"torchvision.transforms.functional_tensor\")\nft.rgb_to_grayscale = F_tvF.rgb_to_grayscale\nsys.modules['torchvision.transforms.functional_tensor'] = ft\nprint(\"Torch:\", torch.__version__, \"| Torchvision:\", torchvision.__version__, \"| Shim OK\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"0d764f5a-fcbc-4f60-a52b-49cff3ec9c0f","cell_type":"code","source":"masknet = UNetSmall().to(device).eval()\nckpt_ema = os.path.join(CKPT_DIR, 'masknet_best_ema.pth')\nckpt     = ckpt_ema if os.path.exists(ckpt_ema) else os.path.join(CKPT_DIR, 'masknet_best.pth')\nif os.path.exists(ckpt):\n    masknet.load_state_dict(torch.load(ckpt, map_location=device), strict=True)\n    print(\"Loaded MaskNet:\", ckpt)\nelse:\n    print(\"[WARN] No MaskNet checkpoint found (demo weights).\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"123b759d-a141-4e0b-9750-e38011a27643","cell_type":"code","source":"from simple_lama_inpainting import SimpleLama\nlama = SimpleLama()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"0295946f-0dc4-4d72-9cba-2dd64f0459cb","cell_type":"code","source":"from gfpgan import GFPGANer\nMODEL_DIR  = \"/kaggle/working/gfpgan_models\"; os.makedirs(MODEL_DIR, exist_ok=True)\nMODEL_PATH = os.path.join(MODEL_DIR, \"GFPGANv1.4.pth\")\nif not os.path.exists(MODEL_PATH):\n    try:\n        print(\"Downloading GFPGANv1.4.pth ...\")\n        urllib.request.urlretrieve(\n            \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\",\n            MODEL_PATH\n        )\n    except Exception as e:\n        print(\"Download failed:\", e, \"\\nUpload the file manually to\", MODEL_PATH)\nrestorer = GFPGANer(model_path=MODEL_PATH if os.path.exists(MODEL_PATH) else None,\n                    upscale=1, arch=\"clean\", channel_multiplier=2, bg_upsampler=None)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"18ec025c-21f7-4e84-9f81-e4bb8c31df46","cell_type":"code","source":"def resize_longest_maxsize_and_pad(img, size=256):\n    h, w = img.shape[:2]\n    if h == 0 or w == 0:\n        return np.zeros((size, size, 3), np.uint8)\n    scale = size / max(h, w)\n    new_w = max(1, int(round(w * scale)))\n    new_h = max(1, int(round(h * scale)))\n    interp = cv2.INTER_AREA if scale < 1 else cv2.INTER_CUBIC\n    resized = cv2.resize(img, (new_w, new_h), interpolation=interp)\n    top = (size - new_h) // 2; bottom = size - new_h - top\n    left = (size - new_w) // 2; right  = size - new_w - left\n    return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_REFLECT_101)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"75220172-f2c6-4821-89de-5107ed31024a","cell_type":"code","source":"@torch.no_grad()\ndef predict_mask(img_rgb_uint8, net, th=0.35, size=SIZE):\n    H, W = img_rgb_uint8.shape[:2]\n    small = resize_longest_maxsize_and_pad(img_rgb_uint8, size=size)\n    x = torch.from_numpy((small.astype(np.float32)/127.5 - 1.0)).permute(2,0,1).to(device)\n    logits = net((x[None]+1)/2)[0,0]\n    pm = torch.sigmoid(logits).float().cpu().numpy()  # [0,1]\n    m_small = (pm>th).astype(np.uint8)*255\n    m = cv2.resize(m_small, (W, H), interpolation=cv2.INTER_NEAREST)\n    k = np.ones((3,3), np.uint8)\n    m = cv2.morphologyEx(m, cv2.MORPH_OPEN, k)\n    m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, k)\n    return m ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"fbca08b0-9138-41fa-8d92-7f75b69fee8a","cell_type":"code","source":"def fix_mask_polarity_for_lama(m_uint8, min_white=0.005, max_white=0.95):\n    wr = (m_uint8 > 127).mean() if m_uint8.size > 0 else 0.0\n    if wr < min_white or wr > max_white:\n        return 255 - m_uint8, True, wr\n    return m_uint8, False, wr","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.697Z"}},"outputs":[],"execution_count":null},{"id":"8d863069-dded-4d7d-8f2b-eb3576da1d78","cell_type":"code","source":"def detect_solid_color_mask(img_rgb_uint8, k=5, area_lo=0.003, area_hi=0.45):\n    H,W = img_rgb_uint8.shape[:2]\n    if H*W < 16: return None\n    lab = cv2.cvtColor(img_rgb_uint8, cv2.COLOR_RGB2LAB)\n    X = lab.reshape(-1,3).astype(np.float32)\n    criteria = (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 50, 0.5)\n    _ret, labels, centers = cv2.kmeans(X, k, None, criteria, 1, cv2.KMEANS_PP_CENTERS)\n    labels = labels.reshape(H,W)\n\n    # gradient trên kênh L (độ sáng)\n    L = lab[:,:,0]\n    gx = cv2.Sobel(L, cv2.CV_32F, 1,0,ksize=3)\n    gy = cv2.Sobel(L, cv2.CV_32F, 0,1,ksize=3)\n    grad = cv2.magnitude(gx,gy)\n    grad = cv2.GaussianBlur(grad, (5,5), 0)\n\n    center = np.zeros((H,W), np.uint8)\n    ch0, ch1 = int(H*0.2), int(H*0.8)\n    cw0, cw1 = int(W*0.2), int(W*0.8)\n    center[ch0:ch1, cw0:cw1] = 1\n\n    best_mask=None; best_score=1e9\n    for lab_id in range(k):\n        m = (labels==lab_id).astype(np.uint8)\n        area = m.mean()\n        if not (area_lo <= area <= area_hi): \n            continue\n        overlap = (m*center).sum() / (center.sum()+1e-6)\n        if overlap < 0.02:  \n            continue\n        mean_grad = float((grad*(m>0)).sum() / (m.sum()+1e-6))\n        if mean_grad < best_score:\n            best_score = mean_grad\n            best_mask = m\n\n    if best_mask is None:\n        return None\n    k5 = np.ones((5,5), np.uint8)\n    m = cv2.morphologyEx(best_mask.astype(np.uint8)*255, cv2.MORPH_OPEN, k5)\n    m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, k5)\n    return m","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"6aecaa2e-af3b-4fc2-99fe-b0e3a0e45d55","cell_type":"code","source":"def inpaint_lama(img_rgb_uint8, mask_uint8_255):\n    from simple_lama_inpainting import SimpleLama\n    return np.array(lama(Image.fromarray(img_rgb_uint8), Image.fromarray(mask_uint8_255)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"fe6c0ca8-ea32-4391-971c-0c153099c8d3","cell_type":"code","source":"def restore_gfpgan(img_rgb_uint8):\n    _, _, restored = restorer.enhance(img_rgb_uint8[:, :, ::-1],\n                                      has_aligned=False, only_center_face=False, paste_back=True)\n    return restored[:, :, ::-1]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"cca867e3-5b42-467d-830e-8767e1329a32","cell_type":"code","source":"def process_image_path(in_path, out_path_mask=None, out_path_inp=None, out_path_rest=None,\n                       out_path_mask_raw=None, out_path_mask_solid=None):\n    img = cv2.imread(in_path)\n    if img is None: raise RuntimeError(f\"Cannot read: {in_path}\")\n    img = img[:, :, ::-1]  # BGR->RGB\n\n    m_pred = predict_mask(img, masknet, th=0.35, size=SIZE)\n\n    m_solid = detect_solid_color_mask(img)\n    if m_solid is not None and out_path_mask_solid:\n        cv2.imwrite(out_path_mask_solid, m_solid)\n\n    m_raw = np.maximum(m_pred, m_solid) if m_solid is not None else m_pred\n\n    m_lama, inverted, wr = fix_mask_polarity_for_lama(m_raw, 0.005, 0.95)\n    if out_path_mask_raw is not None: cv2.imwrite(out_path_mask_raw, m_raw)\n\n    inp  = inpaint_lama(img, m_lama)\n    rest = restore_gfpgan(inp)\n\n    # 6) Lưu\n    if out_path_mask is not None: cv2.imwrite(out_path_mask, m_lama)\n    if out_path_inp  is not None: cv2.imwrite(out_path_inp,  cv2.cvtColor(inp,  cv2.COLOR_RGB2BGR))\n    if out_path_rest is not None: cv2.imwrite(out_path_rest, cv2.cvtColor(rest, cv2.COLOR_RGB2BGR))\n    return img, m_lama, inp, rest\nprint(\"Inference pipeline ready.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"ad909400-0669-4bce-9483-65c277be6a7d","cell_type":"markdown","source":"# Test","metadata":{}},{"id":"092fa946-f937-48bb-8d92-e17af438198f","cell_type":"code","source":"from tqdm.auto import tqdm\nimport os, cv2, numpy as np, glob\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"3532d7c0-6ca1-400b-9f77-22029e5fd8e2","cell_type":"code","source":"def list_images(root):\n    exts = ('.png', '.jpg', '.jpeg', '.bmp', '.webp')\n    return [p for p in glob.glob(os.path.join(root, '**', '*'), recursive=True)\n            if p.lower().endswith(exts)]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"91d4ee6c-e4bc-4383-b6de-afe9aeee7c39","cell_type":"code","source":"INFER_SRC = TEST_DIR if os.path.exists(TEST_DIR) else IMG_DIR\nprint(\"INFER_SRC:\", INFER_SRC)\nos.makedirs(OUT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"10ca7d55-cbfe-463c-bb5c-52845c5c3128","cell_type":"code","source":"def grid4(a,b,c,d):\n    if b.ndim == 2: b3 = np.stack([b]*3, axis=-1)\n    else: b3 = b\n    H = min(a.shape[0], c.shape[0], d.shape[0], b3.shape[0])\n    a  = cv2.resize(a,  (H, H)); b3 = cv2.resize(b3, (H, H))\n    c  = cv2.resize(c,  (H, H)); d  = cv2.resize(d,  (H, H))\n    return np.concatenate([a, b3, c, d], axis=1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"b593e502-ea09-46d8-bf72-f8488cea146e","cell_type":"code","source":"paths = sorted(list_images(INFER_SRC))\nSAMPLE_MAX = None  \nif SAMPLE_MAX is not None: paths = paths[:SAMPLE_MAX]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"da21fdf1-0caa-4141-9182-7b94d94957ca","cell_type":"code","source":"for p in tqdm(paths, desc=\"Infer\"):\n    stem = Path(p).stem\n    uniq = f\"{stem}_{abs(hash(str(Path(p).parent)))%100000}\"\n    out_mask      = os.path.join(OUT_DIR, f\"{uniq}_mask.png\")\n    out_mask_raw  = os.path.join(OUT_DIR, f\"{uniq}_mask_raw.png\")\n    out_mask_sol  = os.path.join(OUT_DIR, f\"{uniq}_mask_solid.png\")\n    out_inp       = os.path.join(OUT_DIR, f\"{uniq}_inpaint.png\")\n    out_res       = os.path.join(OUT_DIR, f\"{uniq}_restored.png\")\n\n    img, m, inp, rest = process_image_path(\n        p, out_path_mask=out_mask, out_path_inp=out_inp, out_path_rest=out_res,\n        out_path_mask_raw=out_mask_raw, out_path_mask_solid=out_mask_sol\n    )\n\n    g = grid4(img, m, inp, rest)\n    cv2.imwrite(os.path.join(OUT_DIR, f\"{uniq}_grid.png\"), cv2.cvtColor(g, cv2.COLOR_RGB2BGR))\n\nprint(\"Done. Check:\", OUT_DIR)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null},{"id":"e002521b-04f8-4b04-a37b-378dfbb312bf","cell_type":"code","source":"import glob, cv2, math\nimport matplotlib.pyplot as plt\nimport os\n\nOUT_DIR = '/kaggle/working/outputs'\npaths = []\nfor ext in ('*.png','*.jpg','*.jpeg','*.webp','*.bmp'):\n    paths.extend(glob.glob(os.path.join(OUT_DIR, ext)))\npaths = sorted(paths)\n\ncols = 2\nrows = math.ceil(len(paths) / cols)\nplt.figure(figsize=(12, rows*5))\nfor i, p in enumerate(paths, 1):\n    img = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)\n    plt.subplot(rows, cols, i)\n    plt.imshow(img); plt.title(os.path.basename(p), fontsize=9); plt.axis('off')\nplt.tight_layout(); plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:53:13.698Z"}},"outputs":[],"execution_count":null}]}